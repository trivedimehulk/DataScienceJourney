{
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# This is assignment submission for \"multiple data science tools\" as part of \"Peer-graded Assignment: Submit Your Work and Grade Your Peers\" by \"Mehul Trivedi\"",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### Date: 7/26/2023",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "#### Introduction",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "##### This note/book should serve the purpose of folowing required items in this assignment:\nExercise 2 - Create a markdown cell with the title of the notebook. (1 pt)\nExercise 3 - Create a markdown cell for an introduction. (1 pt)\nExercise 4 - Create a markdown cell to list data science languages. (3 pts)\nExercise 5 - Create a markdown cell to list data science libraries. (3 pts)\nExercise 6 - Create a markdown cell with a table of Data Science tools. (3 pts)\nExercise 7 - Create a markdown cell introducing arithmetic expression examples. (1 pt)\nExercise 8 - Create a code cell to multiply and add numbers. (2 pts)\nExercise 9 - Create a code cell to convert minutes to hours. (2 pts)\nExercise 10 -Insert a markdown cell to list Objectives. (3 pts)\nExercise 11 - Create a markdown cell to indicate the Authorâ€™s name. (2 pts)\nExercise 12 - Share your notebook through GitHub (3 pts)\nExercise 13 - Take a screenshot of the first page of the notebook. (1 pt)",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "##### List of data science languages\nOut of thousands of different programming languages with their own strengths and weaknesses available for data science journey, depending on your needs, and the problems you are trying to solve, and who you are solving them for, there are options to choose from. \n\nMain recommneded langs are:  Python, R, and SQL.\n\nOther languages which can also be used as need are Scala, Java, C++, and Julia. Considering use cases, you may also opt to use - JavaScript, PHP, Go, Ruby, and Visual Basic.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "##### List of data science libraries\nNote - libraries are collection of functions and code (pre-written) so that you do not reinventh the wheel ;)\n\nHigh level categories of libraries are:\nScientific Computing Libraries in Python\nVisualization Libraries in Python\nHigh-Level- Machine Learning and Deep Learning Libraries \n\nSpeicfic examples:\n\nDat analysis and visualisations:\nPandas - use case: data structures and tools for effective data cleaning, manipulation, and analysis. Mostly 2 dimensional table (col/rows) aka data frame + indexing and other features. Built on topof Numpy.\n\nNumPy - use case: for arrays and matrices, allowing you to apply mathematical functions to the arrays\n\nMatplotlib - use case: for data visualization (well-known library)\n\nSeaborn - use case: for heat maps, time series, and violin plots (inherited from Matplotlib)\n\nMachine learning:\nScikit - use case: for statistical modeling, including regression, classification, clustering etc (built on Numpy + Matplotlib)\nKeras - use case: deep learning\nTensorflow - use case: a low level deep learning lib for production and research use (mostly used in large scale model scenarios)\nPytorch - use case: experimentation and idea building for data scientists and researchers\nApache Spark - use case: general purpose cluster computing framework. The spark lib in this framework provides same functionalities as pandas, numpy and scikit-learn. Apache spark jobs can be based upon: Py or R or Scala\nScala - use case: data engineering and data science:\n - Libraries complimented to Scala framework:\n   - Vegas - use case: statisctical data visulization (works with data files and spark data frames). Deep learning can be done with big DL\nR - use case: built in machine learning and data visulization (works in conjunction with Keras and Tensorflow)\n   ",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "##### Table of Data Science tools.\n|Data Science Tools|Use Case|\n|----|----|\n|MySql, PostgreSQL & NoSQL DBs|Data Management|\n|Hadoop FS, Ceph|File based data Management|\n|Apache Airflow (aka AirbnbKiddo)|Data integration and transformation (cleansing etc)|\n|KubeFlow|Execution of data science pipelines on top of Kubetes|\n|NodeRED|Low processing Visual Editor (RPi)|\n|Kibana|Data exploration and Visulisation (only works with ElasticSearch data source)|\n|Apache Superset|Data exploration and Visulisation|\n\n|Data Science ML Tools|Use Case|\n|----|----|\n|Apache PredictionIO|SparkML only production grade ML deployment env|\n|Seldon|Multi frarmwork support infra (Tensor, Apache, SciKit etc)|\n|MLeap|Multi frarmwork support infra (Tensor, Apache, SciKit etc)|\n\n\n\n",
      "metadata": {}
    }
  ]
}